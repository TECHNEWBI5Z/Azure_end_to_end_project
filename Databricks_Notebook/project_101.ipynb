{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4150a08-c8a9-4982-8b51-67238ce80d90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mounting_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0b87cd-b392-44e5-b55c-94941a73fb72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secrete_adls_key= dbutils.secrets.get(\"faizan-key-vault\", \"faizan-adls-store-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae220ee-c199-4779-9172-b7c6192df0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already mounted: True\nIt is already mounted\n"
     ]
    }
   ],
   "source": [
    "# Initialize the already_mounted variable\n",
    "already_mounted = False\n",
    "\n",
    "# Check if the mount point already exists\n",
    "for x in dbutils.fs.mounts():\n",
    "    if x.mountPoint == \"/mnt/landing_zone\":\n",
    "        already_mounted = True\n",
    "        break  # Exit the loop if the mount point is found\n",
    "print(\"Already mounted:\", already_mounted)\n",
    "\n",
    "# Check if already mounted\n",
    "if not already_mounted:\n",
    "    # Define the storage account name and key\n",
    "    storage_account_name = \"faizanadlsstore\"\n",
    "    storage_account_key = secrete_adls_key\n",
    "    container_name = \"landing-zone\"\n",
    "    mount_point = \"/mnt/landing_zone\"\n",
    "\n",
    "    # Set the configurations\n",
    "    configs = {\n",
    "        \"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_account_name): storage_account_key\n",
    "    }\n",
    "\n",
    "    # Mount the storage\n",
    "    dbutils.fs.mount(\n",
    "        source = \"wasbs://{}@{}.blob.core.windows.net/\".format(container_name, storage_account_name),\n",
    "        mount_point = mount_point,\n",
    "        extra_configs = configs\n",
    "    )\n",
    "    already_mounted = True\n",
    "    print(\"Mounting done successfully\")\n",
    "else:\n",
    "    print(\"It is already mounted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf83606-de86-4ee7-b695-40b31f67f4e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already mounted: True\nIt is already mounted\n"
     ]
    }
   ],
   "source": [
    "# Initialize the already_mounted variable\n",
    "already_mounted = False\n",
    "\n",
    "# Check if the mount point already exists\n",
    "for x in dbutils.fs.mounts():\n",
    "    if x.mountPoint == \"/mnt/lakehouse\":\n",
    "        already_mounted = True\n",
    "        break  # Exit the loop if the mount point is found\n",
    "print(\"Already mounted:\", already_mounted)\n",
    "\n",
    "# Check if already mounted\n",
    "if not already_mounted:\n",
    "    # Define the storage account name and key\n",
    "    storage_account_name = \"faizanadlsstore\"\n",
    "    storage_account_key = secrete_adls_key\n",
    "    container_name = \"lakehouse\"\n",
    "    mount_point = \"/mnt/lakehouse\"\n",
    "\n",
    "    # Set the configurations\n",
    "    configs = {\n",
    "        \"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_account_name): storage_account_key\n",
    "    }\n",
    "\n",
    "    # Mount the storage\n",
    "    dbutils.fs.mount(\n",
    "        source = \"wasbs://{}@{}.blob.core.windows.net/\".format(container_name, storage_account_name),\n",
    "        mount_point = mount_point,\n",
    "        extra_configs = configs\n",
    "    )\n",
    "    already_mounted = True\n",
    "    print(\"Mounting done successfully\")\n",
    "else:\n",
    "    print(\"It is already mounted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe5e1b8-d311-4859-991c-d0cbbb05dbd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## drop table if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246c3389-6c3c-46f8-ad8d-4bf88a228ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping table: growth_marpho_gold\nDropping table: growth_morpho_silver\nDropping table: growth_yield_gold\nDropping table: iot_bronze\nDropping table: iot_gold\nDropping table: iot_silver\nDropping table: onprem_sql_bronze\nDropping table: phy_fst_bronze\nDropping table: phy_fst_gold\nDropping table: phy_fst_silver\nDropping table: phy_scnd_bronze\nDropping table: phy_scnd_gold\nDropping table: phy_scnd_silver\nDropping table: root_fst_bronze\nDropping table: root_fst_gold\nDropping table: root_fst_silver\nDropping table: root_scnd_bronze\nDropping table: root_scnd_gold\nDropping table: root_scnd_silver\nDropping table: yield_and_yield_attribute_silver\nDropping database: growth_lakehouse\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the database name\n",
    "database_name = \"growth_lakehouse\"\n",
    "\n",
    "# Get the list of tables in the database\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "\n",
    "# Drop each table in the database\n",
    "for row in tables_df.collect():\n",
    "    table_name = row.tableName\n",
    "    print(f\"Dropping table: {table_name}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {database_name}.{table_name}\")\n",
    "\n",
    "# Drop the database\n",
    "print(f\"Dropping database: {database_name}\")\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {database_name} CASCADE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05fa4e6-8b93-45d8-9a83-aefe7258e243",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047d59a2-2ec9-4987-9548-175f352c9bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'growth_lakehouse' created.\n"
     ]
    }
   ],
   "source": [
    "# Define the database name\n",
    "database_name = \"growth_lakehouse\"\n",
    "\n",
    "# Check if the database exists\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "database_exists = any(db.databaseName == database_name for db in databases)\n",
    "\n",
    "# Create the database if it doesn't exist\n",
    "if not database_exists:\n",
    "    spark.sql(f\"CREATE DATABASE {database_name}\")\n",
    "    print(f\"Database '{database_name}' created.\")\n",
    "else:\n",
    "    print(f\"Database '{database_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1aaceaf-67b1-418d-8afb-ec0ad7ce068f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create bronze layer folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ee983a-366a-4b90-b6c2-7b4a1f747d95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /mnt/lakehouse/bronze_layer does not exist. Creating it.\nbronze layer folder created\n"
     ]
    }
   ],
   "source": [
    "def create_directory_if_not_exists(directory_path):\n",
    "    try:\n",
    "        # Check if the directory exists by listing its contents\n",
    "        dbutils.fs.ls(directory_path)\n",
    "        print(f\"Directory {directory_path} already exists.\")\n",
    "    except Exception as e:\n",
    "        # If an exception is raised, it means the directory does not exist\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(f\"Directory {directory_path} does not exist. Creating it.\")\n",
    "            dbutils.fs.mkdirs(directory_path)\n",
    "            print(\"bronze layer folder created\")\n",
    "        else:\n",
    "            # Raise the exception if it's a different error\n",
    "            raise e\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/mnt/lakehouse/bronze_layer\"\n",
    "create_directory_if_not_exists(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f73b7d0-a989-46c8-a223-447cc4d6209b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. create bronze layer tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dd4ef8-31ef-4f8f-b99f-a61340fbc4a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Bronze layer for IOT_DATA\n",
    "# Read JSON data with inferred schema\n",
    "iot_hub_df = spark.read.option(\"inferSchema\", True).json(\"dbfs:/mnt/landing_zone/IOT-input/*\")\n",
    "\n",
    "# Repartition the DataFrame to 1 partition and write it as a Delta table\n",
    "iot_hub_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/bronze_layer/iot_hub\").saveAsTable(\"growth_lakehouse.iot_bronze\")\n",
    "\n",
    "# 2. Bronze layer for onprem-sql data \n",
    "onprem_sql_df = spark.read.option(\"inferSchema\", True).csv(\"dbfs:/mnt/landing_zone/onprem-sql-data/dbo.growth_parsed.txt\", header=True)\n",
    "\n",
    "# Repartition the DataFrame to 1 partition and write it as a Delta table\n",
    "onprem_sql_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/bronze_layer/onprem_sql\").saveAsTable(\"growth_lakehouse.onprem_sql_bronze\")\n",
    "\n",
    "# 3. Bronze layer for public access data\n",
    "phy_fst_df = spark.read.option(\"inferSchema\", True).csv(\"dbfs:/mnt/landing_zone/public-data/phy_fst.csv\", header=True)\n",
    "phy_scnd_df = spark.read.option(\"inferSchema\", True).csv(\"dbfs:/mnt/landing_zone/public-data/Phy_snd.csv\", header=True)\n",
    "root_fst_df = spark.read.option(\"inferSchema\", True).csv(\"dbfs:/mnt/landing_zone/public-data/Root_fst.csv\", header=True)\n",
    "root_scnd_df = spark.read.option(\"inferSchema\", True).csv(\"dbfs:/mnt/landing_zone/public-data/Root_scd.csv\", header=True)\n",
    "\n",
    "# Repartition the DataFrames to 1 partition and write them as Delta tables\n",
    "phy_fst_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/bronze_layer/public_access/phy_fst\").saveAsTable(\"growth_lakehouse.phy_fst_bronze\")\n",
    "phy_scnd_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/bronze_layer/public_access/phy_scnd\").saveAsTable(\"growth_lakehouse.phy_scnd_bronze\")\n",
    "root_fst_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/bronze_layer/public_access/root_fst\").saveAsTable(\"growth_lakehouse.root_fst_bronze\")\n",
    "root_scnd_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/bronze_layer/public_access/root_scnd\").saveAsTable(\"growth_lakehouse.root_scnd_bronze\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941ae103-44da-42b0-8230-d113973cbfd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## create silver layer folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67cf2eb5-4668-4a27-b521-362a10e49aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /mnt/lakehouse/silver_layer does not exist. Creating it.\nsilver_layer folder created\n"
     ]
    }
   ],
   "source": [
    "def create_directory_if_not_exists(directory_path):\n",
    "    try:\n",
    "        # Check if the directory exists by listing its contents\n",
    "        dbutils.fs.ls(directory_path)\n",
    "        print(f\"Directory {directory_path} already exists.\")\n",
    "    except Exception as e:\n",
    "        # If an exception is raised, it means the directory does not exist\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(f\"Directory {directory_path} does not exist. Creating it.\")\n",
    "            dbutils.fs.mkdirs(directory_path)\n",
    "            print(\"silver_layer folder created\")\n",
    "        else:\n",
    "            # Raise the exception if it's a different error\n",
    "            raise e\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/mnt/lakehouse/silver_layer\"\n",
    "create_directory_if_not_exists(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcb8cbf-b16f-4cb5-80f9-2e31149ec938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ddb08d-3b8a-43ca-ba79-f88b7f025270",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "import re\n",
    "import random\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3736fc3-c978-4f85-b749-6b6b5fdebfa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### silver_table IOT_Hub data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19695dd0-160a-46ee-b496-1e6114105a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the Delta table into a DataFrame with the provided schema\n",
    "iot_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/bronze_layer/iot_hub/\")\n",
    "\n",
    "# Create or replace the temporary view\n",
    "iot_df.createOrReplaceTempView(\"iot_df\")\n",
    "\n",
    "# Execute the SQL query\n",
    "result_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    messageId,\n",
    "    deviceId,\n",
    "    temperature,\n",
    "    humidity,\n",
    "    soil_ph,\n",
    "    soil_moisture,\n",
    "    irrigation_requirement,\n",
    "    co2_percentage,\n",
    "    light_intensity,\n",
    "    EventProcessedUtcTime,\n",
    "    EventEnqueuedUtcTime\n",
    "FROM iot_df\n",
    "\"\"\")\n",
    "\n",
    "# Function to convert camel case to snake case\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# List of current column names\n",
    "current_columns = result_df.columns\n",
    "\n",
    "# Create a list of new column names\n",
    "new_column_names = [camel_to_snake(c) for c in current_columns]\n",
    "\n",
    "# Create a dictionary mapping old column names to new column names\n",
    "rename_dict = dict(zip(current_columns, new_column_names))\n",
    "\n",
    "# Apply the column renaming using selectExpr\n",
    "renamed_df = result_df.selectExpr([f\"`{old}` AS `{new}`\" for old, new in rename_dict.items()])\n",
    "\n",
    "# Convert date_format for relevant columns\n",
    "for column_name in renamed_df.columns:\n",
    "    if \"date\" in column_name or \"time\" in column_name:\n",
    "        renamed_df = renamed_df.withColumn(\n",
    "            column_name,\n",
    "            date_format(\n",
    "                from_utc_timestamp(\n",
    "                    col(column_name).cast(TimestampType()), \n",
    "                    \"UTC\"\n",
    "                ), \n",
    "                \"yyyy-MM-dd\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Create silver table\n",
    "renamed_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/iot_silver\").saveAsTable(\"growth_lakehouse.iot_silver\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31859fd-63ac-4348-a165-cd1c21f931a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## silver tables of onprem_sql data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c171f9-7190-4d00-893d-64e5688f12b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the Delta table into a DataFrame with the provided schema\n",
    "onprem_sql_silver_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/bronze_layer/onprem_sql/\")\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Function to convert camel case to snake case\n",
    "def camel_to_snake(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# List of current column names\n",
    "current_columns = onprem_sql_silver_df.columns\n",
    "\n",
    "# Create a list of new column names\n",
    "new_column_names = [camel_to_snake(c) for c in current_columns]\n",
    "\n",
    "# Create a dictionary mapping old column names to new column names\n",
    "rename_dict = dict(zip(current_columns, new_column_names))\n",
    "\n",
    "# Apply the column renaming using selectExpr\n",
    "renamed_df = onprem_sql_silver_df.selectExpr([f\"`{old}` AS `{new}`\" for old, new in rename_dict.items()])\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "renamed_df.createOrReplaceTempView(\"growth_renamed\")\n",
    "#data_segregation\n",
    "yield_and_yield_attribute_df = spark.sql(\"\"\" \n",
    "                                         SELECT \n",
    "                                        genotype_name, plant_height_30_dat_cm, plant_height_60_dat_cm, plant_height_90_dat_cm, \n",
    "nu_primery_branches__num, nu_fruits_per_plant, avg_fruit_weight_gram, days_flower_initiation_days, \n",
    "fruit_tss__brix, fruit_circumference_mm, fruit_length_cm, fruit_yield_per_plant__kg, yield_status\n",
    "from growth_renamed\n",
    "                                         \"\"\")\n",
    "\n",
    "growth_morpho_df = spark.sql(\"\"\" \n",
    "                                         SELECT \n",
    "                               genotype_name, stem_anthocyanin, anthocyanin_intensity, stem_pubescence, stem_pubescence_intensity, \n",
    "leaf_blade_colour, leaf_blade_colour_intensity, colour_of_vein, intensity_of_colour8, spineon_leaf, \n",
    "flower_colour, fruiting_pattern, fruit_colour, intensityof_colour13, stripes, stripes_density, \n",
    "fruit_patches, spines_in_calyx, density_of_spininess, fruit_shape\n",
    "from growth_renamed\n",
    "                                         \"\"\")\n",
    "\n",
    "# Rename columns in the DataFrame\n",
    "growth_morpho_df = growth_morpho_df = growth_morpho_df \\\n",
    "    .withColumnRenamed(\"intensityof_colour13\", \"intensity_of_colour\") \\\n",
    "    .withColumnRenamed(\"intensity_of_colour8\", \"intensity_of_colour_z\")\n",
    "\n",
    "\n",
    "# Create silver table\n",
    "yield_and_yield_attribute_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/yield_and_yield_attribute_silver\").saveAsTable(\"growth_lakehouse.yield_and_yield_attribute_silver\")\n",
    "\n",
    "growth_morpho_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/growth_morpho_silver\").saveAsTable(\"growth_lakehouse.growth_morpho_silver\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbe0bf4-f941-475e-8b8a-10993cb2df14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## silver table for public access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64958f82-b15e-403f-9818-e579b8b20c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to convert camel case to snake case\n",
    "def camel_to_snake(name: str) -> str:\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# Function to rename DataFrame columns from camel case to snake case\n",
    "def rename_columns_to_snake_case(df: DataFrame) -> DataFrame:\n",
    "    new_columns = [camel_to_snake(col) for col in df.columns]\n",
    "    return df.toDF(*new_columns)\n",
    "\n",
    "# Read DataFrames\n",
    "phy_fst_silver_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/bronze_layer/public_access/phy_fst/\")\n",
    "phy_scnd_silver_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/bronze_layer/public_access/phy_scnd/\")\n",
    "root_fst_silver_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/bronze_layer/public_access/root_fst/\")\n",
    "root_scnd_silver_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/bronze_layer/public_access/root_scnd/\")\n",
    "\n",
    "# Rename columns to snake case\n",
    "phy_fst_silver_df = rename_columns_to_snake_case(phy_fst_silver_df)\n",
    "phy_scnd_silver_df = rename_columns_to_snake_case(phy_scnd_silver_df)\n",
    "root_fst_silver_df = rename_columns_to_snake_case(root_fst_silver_df)\n",
    "root_scnd_silver_df = rename_columns_to_snake_case(root_scnd_silver_df)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# write back into silver tables\n",
    "phy_fst_silver_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/public_acess/phy_fst_silver\").saveAsTable(\"growth_lakehouse.phy_fst_silver\")\n",
    "phy_scnd_silver_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/public_acess/phy_scnd_silver\").saveAsTable(\"growth_lakehouse.phy_scnd_silver\")\n",
    "root_fst_silver_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/public_acess/root_fst_silver\").saveAsTable(\"growth_lakehouse.root_fst_silver\")\n",
    "root_scnd_silver_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/silver_layer/public_acess/root_scnd_silver\").saveAsTable(\"growth_lakehouse.root_scnd_silver\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8b36a0-8744-4f7a-982e-05958e383da6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## creating gold layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74f75a6-39cb-4e04-b700-2f2b15f701b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## create gold layer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257dbc9d-d193-41ca-b17d-014d11be2396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /mnt/lakehouse/gold_layer does not exist. Creating it.\ngold_layer folder created\n"
     ]
    }
   ],
   "source": [
    "def create_directory_if_not_exists(directory_path):\n",
    "    try:\n",
    "        # Check if the directory exists by listing its contents\n",
    "        dbutils.fs.ls(directory_path)\n",
    "        print(f\"Directory {directory_path} already exists.\")\n",
    "    except Exception as e:\n",
    "        # If an exception is raised, it means the directory does not exist\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(f\"Directory {directory_path} does not exist. Creating it.\")\n",
    "            dbutils.fs.mkdirs(directory_path)\n",
    "            print(\"gold_layer folder created\")\n",
    "        else:\n",
    "            # Raise the exception if it's a different error\n",
    "            raise e\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/mnt/lakehouse/gold_layer\"\n",
    "create_directory_if_not_exists(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b585df8-dcbb-4117-a4b7-c9ca82d0a679",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## gold table for growth yield and yield attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9953542-23e3-4ce7-9586-0d62004c9756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "growth_df = spark.read.format('delta').load(\"dbfs:/mnt/lakehouse/silver_layer/yield_and_yield_attribute_silver\")\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#Type casting\n",
    "# List of columns to be rounded to two decimal places\n",
    "columns_to_round = ['plant_height_30_dat_cm', 'plant_height_60_dat_cm', 'plant_height_90_dat_cm',\n",
    "                    'avg_fruit_weight_gram', 'days_flower_initiation_days', 'fruit_tss__brix', 'nu_primery_branches__num', 'nu_fruits_per_plant',\n",
    "                    'fruit_circumference_mm', 'fruit_length_cm', 'fruit_yield_per_plant__kg']\n",
    "\n",
    "# Apply rounding to specified columns\n",
    "for column in columns_to_round:\n",
    "    growth_df = growth_df.withColumn(column, round(col(column), 2))\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# adding polyhouse_number for reference as (randomly generated without any background verification)\n",
    "\n",
    "# Function to generate random polyhouse numbers\n",
    "def generate_random_polyhouse():\n",
    "    return f\"G{str(random.randint(1, 10)).zfill(2)}\"\n",
    "\n",
    "# Register UDF\n",
    "generate_random_polyhouse_udf = udf(generate_random_polyhouse, StringType())\n",
    "\n",
    "# Add new column with random polyhouse numbers\n",
    "growth_df = growth_df.withColumn(\"polyhouse_number\", generate_random_polyhouse_udf())\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#  createing surrogated key \n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(\"genotype_name\")\n",
    "\n",
    "# Add the SK1 column with sequential row numbers\n",
    "growth_gold_df = growth_df.withColumn(\"SK1\", row_number().over(window_spec))\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#  write gold delta table\n",
    "growth_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/growth_yield_gold\").saveAsTable(\"growth_lakehouse.growth_yield_gold\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2f6500-d8b4-452f-8bd6-57d6bbaba53d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## gold table for growth marpho attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bf5242-ba2e-4571-85f4-0f45094cd5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "growth_marpho_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/silver_layer/growth_morpho_silver\")\n",
    "\n",
    " #-------------------------------------------------------\n",
    "#  create surrogated key\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(\"genotype_name\")\n",
    "\n",
    "# Add the SK1 column with sequential row numbers\n",
    "growth_marpho_gold_df = growth_marpho_df.withColumn(\"SK1\", row_number().over(window_spec))\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#  write gold delta table\n",
    "\n",
    "growth_marpho_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/growth_marpho_gold\").saveAsTable(\"growth_lakehouse.growth_marpho_gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11022ec6-517f-4808-9d47-1cf855fdc224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## gold table for IOT_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b559b7-11ce-418e-818e-0b8ca0786dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "IOT_DATA_DF = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/silver_layer/iot_silver\")\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# type casting\n",
    "\n",
    "# List of columns to round\n",
    "columns = ['temperature', 'humidity']\n",
    "\n",
    "# Iterate over each column and round its values to 2 decimal places\n",
    "renamed_df = IOT_DATA_DF\n",
    "for cola in columns:\n",
    "    renamed_df = renamed_df.withColumn(cola, round(col(cola), 2))\n",
    "\n",
    "\n",
    "# add polyhouse_number\n",
    "# Add new column with random polyhouse numbers\n",
    "IOT_gold_df = renamed_df.withColumn(\"polyhouse_number\", generate_random_polyhouse_udf())\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# write the gold table\n",
    "\n",
    "IOT_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/IOT_gold\").saveAsTable(\"growth_lakehouse.IOT_gold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712a6512-5604-411d-9dba-b8508d0d3121",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## gold table for physiological and root data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edb9d81-a03d-4342-a5cf-eb942e204089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "phy_fst_gold_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/silver_layer/public_acess/phy_fst_silver\")\n",
    "phy_scnd_gold_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/silver_layer/public_acess/phy_scnd_silver\")\n",
    "root_fst_gold_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/silver_layer/public_acess/root_fst_silver\")\n",
    "root_scnd_gold_df = spark.read.format(\"delta\").load(\"dbfs:/mnt/lakehouse/silver_layer/public_acess/root_scnd_silver\")\n",
    " #-------------------------------------------------------\n",
    "#  create surrogated key\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(\"genotype_name\")\n",
    "\n",
    "# Add the SK1 column with sequential row numbers\n",
    "phy_fst_gold_df = phy_fst_gold_df.withColumn(\"SK1\", row_number().over(window_spec))\n",
    "phy_scnd_gold_df = phy_scnd_gold_df.withColumn(\"SK1\", row_number().over(window_spec))\n",
    "root_fst_gold_df = root_fst_gold_df.withColumn(\"SK1\", row_number().over(window_spec))\n",
    "root_scnd_gold_df = root_scnd_gold_df.withColumn(\"SK1\", row_number().over(window_spec))\n",
    "\n",
    "phy_fst_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/phy_fst_gold\").saveAsTable(\"growth_lakehouse.phy_fst_gold\")\n",
    "\n",
    "phy_scnd_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/phy_scnd_gold\").saveAsTable(\"growth_lakehouse.phy_scnd_gold\")\n",
    "\n",
    "root_fst_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/root_fst_gold\").saveAsTable(\"growth_lakehouse.root_fst_gold\")\n",
    "\n",
    "root_scnd_gold_df.repartition(1).write.mode(\"overwrite\").format(\"delta\").option(\"path\", \"/mnt/lakehouse/gold_layer/root_scnd_gold\").saveAsTable(\"growth_lakehouse.root_scnd_gold\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 570422339453625,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "project_101",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
